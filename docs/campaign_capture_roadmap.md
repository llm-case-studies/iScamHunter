# Campaignâ€‘Capture Extension: Threeâ€‘Phase Roadmap & Current Status

ğŸ“– **Overview & Highâ€‘Level Goals**

We are building a Chrome extension that enables an entire team to document, archive, and migrate campaign assetsâ€”including websites (store, funnel), social profiles (Facebook, Instagram, Telegram, YouTube), and user workflowsâ€”into a single, unified representation. The final deliverable is:

* Offline archives of every pageâ€™s HTML (with images inlined), outline (DOM summary), text, and stitched screenshots.
* Userâ€‘driven â€œdemo pathsâ€ logged as clickâ€‘streams, merged with a crawlerâ€‘generated site map, producing a comprehensive link graph.
* Visual flowcharts (Mermaidâ€‘formatted) depicting each campaign assetâ€™s structure and how users navigate between them.

To achieve this, weâ€™ll work in three successive phases. Each phase has clear stepping stones (not just milestones) and builds directly on the previous phase. At the end of PhaseÂ 1 (this chat), the MVPâ€”HTML/JSON/Text dumps + fullâ€‘page stitched screenshot + basic popup UIâ€”has been implemented. PhasesÂ 2 andÂ 3 remain.

âœ… **PhaseÂ 1 (Completed in ChatÂ #1)**

Objective: Deliver an MVP that can capture any single page (on any domain) and save all of:

* HTML dump (raw outerHTML, with external assets still referenced)
* Outline JSON (a simple DOMâ€‘outline array)
* Text dump (document.body.innerText)
* Fullâ€‘page stitched PNG screenshot

**Deliverables:**

* **capture.js (content script)**

  * Listens for `CAPTURE_NOW`, `GET_DIMENSIONS`, `SCROLL_TO`, `DETECT_PANES`.
  * Returns `{ html, outline, text }` on `CAPTURE_NOW`.
  * Reports page dimensions and scrolls upon request.
  * Detects all scrollable elements (`el.scrollHeight - el.clientHeight > 20`).

* **screenshot.js (helper module)**

  * Implements `fullPageCapture(tab, baseName, { saveTiles, imgType })`.
  * Throttles `captureVisibleTab()` calls to respect Chromeâ€™s quota.
  * Uses `createImageBitmap()` and an `OffscreenCanvas` to stitch multiple viewport captures into one tall PNG.
  * Converts the final Blob into a dataâ€‘URL for `chrome.downloads.download`.

* **background.js (service worker)**

  * Imports `screenshot.js` and `logger.js`.
  * Listens for the popupâ€™s `CAPTURE_WITH_OPTIONS` message.
  * Sends `CAPTURE_NOW` to the active tab, then saves whichever of `[html, outline, text]` the user requested.
  * If â€œscreenshotâ€ is requested, calls `fullPageCapture(...)` with `saveTiles: false`.
  * Logs warnings for paneâ€‘capture requests (not yet implemented).

* **popup.html & popup.js**

  * Renders checkboxes for Main window: Screenshot / Outline / HTML / Text.
  * â€œAllâ€ and â€œNoneâ€ buttons toggle all checkboxes.
  * On â€œStart Capture,â€ collects `options.main = [ ... ]` and sends to background.
  * Dynamically requests `DETECT_PANES` to list scrollable panes, rendering four checkboxes per pane (Screenshot / Outline / HTML / Text).
  * On â€œStart Capture,â€ also packages each paneâ€™s selected actions into `options['paneId'] = [ ... ]`.

* **logger.js**

  * Provides `logger.info()`, `logger.debug()`, `logger.warn()`, `logger.error()`, prefixed with `[I]/[D]/[W]/[E]` aide:.
  * Allows centralized logâ€‘level control if needed in future.

* **manifest.json**

  ```json
  {
    "content_scripts": [
      { "matches": ["<all_urls>"], "js": ["logger.js","capture-utils.js","screenshot.js","capture.js"], "run_at": "document_end" }
    ],
    "background": { "service_worker": "background.js", "type": "module" },
    "action": { "default_popup": "popup.html" },
    "permissions": ["downloads","activeTab","tabs","scripting"]
  }
  ```

**Verified Behavior:**

* Browsing to any page, clicking the extension icon or using the popup â†’ downloads four files (if all checkboxes checked):

  * `Capture_<hostname>_<timestamp>.html`
  * `Capture_<hostname>_<timestamp>.json` (outline)
  * `Capture_<hostname>_<timestamp>.txt` (text)
  * `Capture_<hostname>_<timestamp>_full.png` (stitched screenshot)
* No runtime errors (Chrome quotas respected; no "Image is not defined"; no `URL.createObjectURL` failures).
* Pane detection is functional, but perâ€‘pane capture was stubbed (logs a warning).

ğŸ¯ **PhaseÂ 2: â€œPageâ€‘Level Polish & Pane Refinementâ€**

**Goal:** Elevate the MVP into a robust pageâ€‘archival tool by adding:

* **Accurate Pane Filtering** so that the popup only lists truly meaningful, topâ€‘level scrollable regions (no â€œimage wrappersâ€ or nested `<div>` ghosts).
* **Image Inlining in Saved HTML** so that every `<img>` is embedded as a dataâ€‘URI, producing a fully selfâ€‘contained HTML snapshot (no external image URLs).
* **â€œSave Tilesâ€ Toggle** in the popup that, when checked, instructs the screenshot routine to download every intermediate tile (e.g. `â€¦_tile_000.png`, `â€¦_tile_001.png`) rather than just the final `_full.png`.
* **Optional:** Paneâ€‘Level Captures (HTML/Text/Screenshot) for each scrollable region, if time permits.

ğŸ” **PhaseÂ 2.1: Pane Filtering**

**Current Logic (PhaseÂ 1):**

```js
const panes = [...document.querySelectorAll('*')]
  .filter(el => el.scrollHeight - el.clientHeight > 20)
  .map(...);
```

This finds every element whose content overflows by >Â 20pxâ€”including deeply nested `.image-wrapper` or hidden containers.

**Desired Logic:**

* Only topâ€‘level scrollable containers: Exclude any element that has a scrollable ancestor already in the list.
* Skip `<html>`/`<body>` if theyâ€™re treated separately as the â€œMain Window.â€
* Optionally enforce a minimum absolute height (e.g. `scrollHeight > 800px`) to capture only large regions.
* Skip nodes with `overflow: hidden` or `visibility: hidden` (CSSâ€‘computed style check).

**Implementation Steps:**

* Modify `capture.js`â€™s `DETECT_PANES` handler:

  ```js
  if (msg.type === 'DETECT_PANES') {
    const allOverflow = [...document.querySelectorAll('*')]
      .filter(el =>
        el.scrollHeight - el.clientHeight > 20 &&
        el !== document.documentElement &&
        el !== document.body &&
        window.getComputedStyle(el).overflowY !== 'hidden' &&
        el.scrollHeight > 300 // example threshold
      );
    // Remove nested children if parent is already in allOverflow
    const topLevel = allOverflow.filter((el) =>
      !allOverflow.some((parent) => parent !== el && parent.contains(el))
    );
    const panes = topLevel.map((el, i) => ({
      id: `pane_${i}`,
      label: el.id || el.className.split(' ')[0] || el.tagName.toLowerCase(),
      height: el.scrollHeight
    }));
    sendRes(panes);
    return true;
  }
  ```

* Test on pages with multiple nested scrollable `<div>`s (e.g. Facebook feed) to confirm only one or two entries appear.

* Update `popup.js` to render only these filtered panes under â€œScrollable Panes.â€

ğŸ” **PhaseÂ 2.2: Image Inlining**

**Objective:** Before returning `outerHTML` on `CAPTURE_NOW`, replace every `<img src="â€¦">` with a `data:image/...;base64,â€¦` URL so that the saved HTML is fully selfâ€‘contained.

**Implementation Steps:**

* In `capture.js`, add a helper function:

  ```js
  async function inlineAllImages() {
    // Collect all <img> tags
    const imgs = Array.from(document.querySelectorAll('img[src]'));
    // For each <img>, fetch its blob, convert to Data URL, then replace src
    await Promise.all(imgs.map(async (img) => {
      try {
        // Use fetch with credentials to handle cookies if crossâ€‘origin
        const resp = await fetch(img.src, { credentials: 'include' });
        if (!resp.ok) return;
        const blob = await resp.blob();
        const dataUrl = await new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onload = () => resolve(reader.result);
          reader.onerror = reject;
          reader.readAsDataURL(blob);
        });
        img.src = dataUrl;
      } catch (e) {
        console.warn('inlineAllImages: failed to inline', img.src, e);
      }
    }));
  }
  ```

* Modify the `CAPTURE_NOW` handler:

  ```js
  if (msg.type === 'CAPTURE_NOW') {
    // 1) Inline images if HTML is requested
    if (msg.options?.main?.includes('html')) {
      await inlineAllImages();
    }
    // 2) Capture HTML, Outline, Text as before
    const html = document.documentElement.outerHTML;
    const outline = buildOutline(document.body);
    const text = document.body.innerText || '';
    sendRes({ html, outline, text });
    return true;
  }
  ```

* Update `background.js` to send options along with `CAPTURE_NOW`:

  ```js
  // Instead of chrome.tabs.sendMessage(tab.id, { type: 'CAPTURE_NOW' });
  const resp = await chrome.tabs.sendMessage(tab.id, {
    type: 'CAPTURE_NOW',
    options: opts  // so capture.js knows whether to inline images
  });
  ```

* Verify by capturing a page with many `<img>` tags (e.g. an Instagram post) and open `Capture_<host>.html` offlineâ€”images must appear embedded.

ğŸ” **PhaseÂ 2.3: â€œSave Tilesâ€ UI & Logic**

**Objective:** Let users choose whether to download each tiled viewport image in addition to (or instead of) a single stitched fullâ€‘page PNG.

**Popup UI Changes (`popup.html` / `popup.js`):**

* Under â€œMain windowâ€ checkboxes, add:

  ```html
  <input type="checkbox" id="main-tiles" data-id="tiles" />
  <label for="main-tiles">Save tiles</label>
  ```

* In `popup.js`, when gathering options:

  ```js
  const mainModes = [...document.querySelectorAll('#main-section input[type=checkbox]:checked')]
    .map(el => el.dataset.id)
    .filter(id => id !== 'tiles');  // â€œtilesâ€ isnâ€™t a mode but a flag
  options.main = mainModes;  
  options.saveTiles = document.querySelector('#main-tiles').checked;
  // Send saveTiles in the CAPTURE_WITH_OPTIONS message.
  ```

**Background Logic (`background.js`):**

* In `runCaptureWithOptions(tab, opts)`, change:

  ```js
  // if mainOpts includes 'screenshot'
  if (mainOpts.includes('screenshot')) {
    const saveTiles = !!opts.saveTiles;
    await fullPageCapture(tab, base, { saveTiles, imgType: 'png' });
  }
  ```

* Confirm that if `saveTiles === true`, each tile is downloaded as `â€¦_tile_000.png`, etc. When `saveTiles === false`, only `â€¦_full.png` is emitted.

**Testing:**

* Capture a tall page (e.g. `/store`) with â€œScreenshotâ€ checked but â€œSave tilesâ€ uncheckedâ€”verify only a single `_full.png` appears.
* Capture again with â€œSave tilesâ€ checkedâ€”verify multiple `_tile_000.png`, `_tile_001.png`, â€¦ plus optionally `_full.png` (or, if you choose, skip `_full.png` and keep only tiles).
* Fix any filenameâ€‘collision issues (e.g. tile and fullâ€‘page naming).

ğŸ” **PhaseÂ 2.4: (Optional Stretch) Perâ€‘Pane HTML/Text/Screenshot**

If time permits, implement actual paneâ€‘capture instead of logging a warning. For each detected pane (from `DETECT_PANES`), allow the user to request:

* HTML dump of that pane only (by scoping `document.querySelector(paneSelector).outerHTML`).
* Text dump of that pane only (paneElement.innerText).
* Screenshot of that pane only (scroll that pane to top, use `captureVisibleTab` with cropping/clipping to the paneâ€™s bounding box, or clone the pane into a temporary fullâ€‘viewport container and capture).

Because pane logic can become complex, you may decide to defer it to PhaseÂ 3. In PhaseÂ 2, itâ€™s acceptable to keep â€œperâ€‘pane = unimplemented (warning).â€

**Commit & Document PhaseÂ 2**

At completion, write a brief â€œPhaseÂ 2 Handoffâ€ note (e.g. `docs/phase2-handoff.md`) listing:

* Files changed (`capture.js`, `popup.html`, `popup.js`, `background.js`, etc.).
* What was accomplished (pane filtering, image inlining, saveâ€‘tiles toggle).
* What remains (perâ€‘pane capture, demo path).

ğŸ”¹ **PhaseÂ 3: â€œDemo Paths, Multiâ€‘Domain Crawl & Mermaid Flowchartsâ€**

**Goal:** Combine humanâ€‘driven demo sessions with an automated crawler that spans the store/funnel site and all social media assets, then visualize the full campaign as a Mermaid flowchart.

ğŸ“Œ **Key Capabilities to Add**

* **Demoâ€‘Path Logging**

  * Record the exact sequence of URLs a team member visits (â€œHome â†’ Sales â†’ Checkout â†’ FB Page â†’ IG Profile â†’ â€¦â€).
  * Store this ordered list in memory (e.g. `demoPath = []`) within `interaction-logger.js`.
  * Provide a `GET_DEMO_PATH` message to retrieve that list when the user requests it.

* **PhaseÂ B Crawl Seeded by Demo Path**

  * Take the `demoPath` array of URLs and use it as the seed queue for a hiddenâ€‘tab crawl (PhaseÂ B).
  * For every URL in `demoPath`, open a hidden tab:

    1. Wait for `document.readyState === 'complete'`.
    2. Programmatically scroll to the bottom (to load infinite content).
    3. Extract all sameâ€‘campaignâ€‘domain `<a href>` links (Facebook, Instagram, Telegram, YouTube, your store).
    4. Enqueue newly discovered links if they match any of your whitelisted campaign domains.
    5. Continue until the queue is exhausted or a page/iteration limit is reached.

* **PhaseÂ A (Optional) Fast Fetch Crawl**

  * (If desired) Run a parallel, pureâ€‘fetch + `DOMParser` crawl on each demo URL to quickly harvest all serverâ€‘rendered `<a>` links.
  * Merge with PhaseÂ B results to catch both static and dynamic links.

* **Edge & Node Extraction**

  * **Demo edges:** for every consecutive pair in `demoPath`:

    ```js
    demoEdges = demoPath.slice(0,-1).map((u,i) => [u, demoPath[i+1]]);
    ```
  * **Autoâ€‘discovered edges:** for each crawled page `u`, for each extracted sameâ€‘host link `v`, record `[u, v]`.
  * Merge edges (remove duplicates) to form a final edge list.

* **Mermaidâ€‘Formatted Flowchart Generation**

  1. Assign each unique URL a short ID (`N1, N2, N3â€¦`).
  2. Build node declarations (e.g. `N1["Home"]`, using pathname or `document.title` as a label).
  3. Build arrow declarations for every edge (`N1 --> N2`).
  4. Optionally group nodes into subgraphs (e.g. â€œsubgraph Facebook { F1\[FBâ€‘Page] â€¦ }â€).
  5. Output the entire text block:

  ```mermaid
  flowchart LR
    N1["Home"] --> N2["Sales"]
    N2 --> N3["Checkout"]
    N3 --> F1["FBâ€‘Page"]
    F1 --> I1["IG Profile"]
    I1 -->[] N2  <!-- IG links back to /sales -->
    â€¦
  ```

  5. Provide a â€œDownload Flowchartâ€ button in the popup that saves this `.mmd` file.

* **Multiâ€‘Domain Whitelisting**

  ```js
  const CAMPAIGN_DOMAINS = [
    "iscamhunter.com", "www.iscamhunter.com",
    "facebook.com",     "www.facebook.com",
    "instagram.com",    "www.instagram.com",
    "t.me",             "telegram.me",
    "youtube.com",      "www.youtube.com"
  ];
  ```

  * During link extraction (both in page and in hidden tabs), only enqueue hrefs whose `new URL(href).hostname` is in that list.

* **UI in Popup**
  Add:

  ```html
  <button id="start-demo-btn">Start Demo</button>
  <button id="clear-demo-btn">Clear Demo</button>
  <button id="build-flow-btn">Build Campaign Flowchart</button>
  ```

  `popup.js`:

  ```js
  document.getElementById('start-demo-btn').addEventListener('click', () => {
    chrome.tabs.sendMessage(activeTabId, { type: 'CLEAR_DEMO_PATH' });
    appendLog('Demo path reset. Start navigating now.');
  });

  document.getElementById('clear-demo-btn').addEventListener('click', () => {
    chrome.tabs.sendMessage(activeTabId, { type: 'CLEAR_DEMO_PATH' });
    appendLog('Demo path cleared.');
  });

  document.getElementById('build-flow-btn').addEventListener('click', () => {
    // 1) Retrieve demoPath
    chrome.tabs.sendMessage(activeTabId, { type: 'GET_DEMO_PATH' }, (demoPath) => {
      // 2) Kick off combined crawl
      chrome.runtime.sendMessage(
        { type: 'START_COMBINED_CRAWL_WITH_DEMO', demoPath },
        (resp) => {
          if (resp.success) {
            // 3) Download .mmd
            const filename = `CampaignFlow_${new Date().toISOString().replace(/[:.]/g, '-')}.mmd`;
            chrome.downloads.download({
              url: 'data:text/plain;charset=utf-8,' + encodeURIComponent(resp.mermaid),
              filename
            });
            appendLog(`Flowchart downloaded as ${filename}`);
          } else {
            appendLog(`Error building flowchart: ${resp.error}`);
          }
        }
      );
    });
  });
  ```

  `background.js`: Add listener for `START_COMBINED_CRAWL_WITH_DEMO`, run the twoâ€‘phase crawl + edge extraction + `buildMermaid`, then return `{ success: true, mermaid: â€¦ }`.

**Testing & Validation**

* **Demo Recording:**

  1. Click â€œStart Demo.â€
  2. Navigate the entire funnel: Home â†’ Mat Page â†’ Checkout â†’ Order Confirmation â†’ FB Page â†’ IG Profile â†’ YouTube Channel.
  3. Ensure that each URL change is appended to `demoPath`.
  4. Click â€œBuild Campaign Flowchart.â€

* **Hiddenâ€‘Tab Crawl:**

  * Confirm that background spawns hidden tabs for every URL in the `demoPath`.
  * Each hidden tab scrolls and extracts all allowedâ€‘host `<a>` links (including subsequent posts or related pages on social platforms).
  * Each hidden tab closes after finishing.

* **Edge Merging:**

  * Verify that `demoEdges` (the humanâ€™s consecutive clicks) appear as solid arrows.
  * Verify that `autoEdges` (any other links found on each page) appear as dashed arrows or a separate style if you choose.
  * The final Mermaid file should reflect both.

* **Mermaid Output:**

  * Paste the downloaded `.mmd` into mermaid.live or any Mermaidâ€‘enabled Markdown viewer.
  * Confirm readability: nodes labeled by path or page title, distinct subgraphs per domain if you implemented them.
  * Optionally test grouping:

    ```mermaid
    flowchart LR
      subgraph Store
        N1["Home"] --> N2["Products"]
        N2 --> N3["Checkout"]
      end
      subgraph Facebook
        N4["FB Page"] --> N5["FB Post #1"]
        N5 --> N1
      end
    ```

* **Edge Cases:**

  * If a hidden tab fails to load, log a warning and continue.
  * If `demoPath` is empty, prompt the user to â€œclick Start Demoâ€ and navigate first.
  * If no crawl results beyond the demo path, still produce a valid flowchart containing just the demo edges.

**Commit & Document PhaseÂ 3**

Create `docs/phase3-handoff.md` summarizing:

* All files and methods added (`interaction-logger.js` updates, `crawler-content.js`, `background.js` changes, `popup.html/js` updates).
* How to run the new buttons.
* What the final deliverables look like.

ğŸŒ± **Summary of Completed Work in This Chat (PhaseÂ 1)**

Implemented MVP extension capable of:

* Capturing raw HTML, outline JSON, and plain text for any single page.
* Generating and downloading a fullâ€‘page stitched PNG without errors.
* Providing a popup UI with â€œMain Windowâ€ checkboxes and a list of detected panes (though paneâ€‘capture remains a stub).
* Splitting code into modules (`capture.js`, `screenshot.js`, `background.js`, `popup.html/js`, `logger.js`, `capture-utils.js`).
* Logging via a centralized logger helper.
* Committing the full codebase to a GitHub repository, tagged as â€œPhase 1 complete.â€

ğŸš© **Next Steps: Handover to ChatÂ #2**

When you start ChatÂ #2, please supply:

* **â€œPhaseÂ 1 Handoffâ€ summary** (the bullet list above) so ChatÂ #2 knows exactly what has been built.
* **All relevant files under** `Browser_Addins/General/Chrome` (e.g. `background.js`, `capture.js`, `screenshot.js`, `popup.html`, `popup.js`, `logger.js`, `capture-utils.js`, `manifest.json`, `icons`). You can either upload the folder or copyâ€‘paste individual files.

With that in place, ChatÂ #2 will have full context to implement PhaseÂ 2 stepping stones (pane filtering, image inlining, saveâ€‘tiles toggle).

---

*End of Mission Statement & Roadmap*

* PhaseÂ 1 is done.
* PhaseÂ 2 assignments and milestones outlined above.
* PhaseÂ 3 assignments and milestones outlined above.
